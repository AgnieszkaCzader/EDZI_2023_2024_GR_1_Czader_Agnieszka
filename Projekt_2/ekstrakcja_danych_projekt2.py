# -*- coding: utf-8 -*-
"""ekstrakcja_danych_projekt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10W51YKw1iBeFUsy02ZO5h2HQJseRBYdK
"""

import json
import pandas as pd
!pip install googletrans==4.0.0-rc1
!pip install sumy
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer
from googletrans import Translator
import nltk
nltk.download('punkt')
import requests
import csv
import time

from google.colab import drive
# Podłączenie dysku Google
drive.mount('/content/drive')

"""**Tabele wymiarów**"""

# Wczytanie danych
with open("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/job_details_pracuj.json", "r") as file:
    job_details = json.load(file)

# DataFrame
df = pd.DataFrame(job_details)

# Unikalne wartości Array
skills_df = df["Array"].explode().reset_index(drop=True).drop_duplicates().reset_index(drop=True).to_frame(name="umiejętności")

# Usunięcie duplikatów
skills_df = skills_df.drop_duplicates().reset_index(drop=True)

# Dodanie ID
skills_df["ID Umiejętności"] = skills_df.index + 1

# Rozbijanie na osobne wiersze
df = df.explode("Array").reset_index(drop=True)

# Usunięcie duplikatów
df.drop_duplicates(subset=["Array"], inplace=True)

# Połączenie DataFrame'ów
df = pd.merge(df, skills_df, left_on="Array", right_on="umiejętności")

# Usunięcie kolumny Array
df.drop(columns=["Array"], inplace=True)

# Tworzenie tabel unikalnych wartości
stanowiska_df = df[["stanowisko"]].drop_duplicates().reset_index(drop=True)
firmy_df = df[["firma"]].drop_duplicates().reset_index(drop=True)
kategorie_df = df[["kategoria"]].drop_duplicates().reset_index(drop=True)
waluty_df = df[["waluta"]].drop_duplicates().reset_index(drop=True)
źródła_df = df[["źródło"]].drop_duplicates().reset_index(drop=True)

# Dodawanie ID
stanowiska_df["ID Stanowiska"] = stanowiska_df.index + 1
firmy_df["ID Firmy"] = firmy_df.index + 1
waluty_df["ID Waluty"] = waluty_df.index + 1
źródła_df["ID Źródła"] = źródła_df.index + 1
kategorie_df["ID Kategoria"] = kategorie_df.index + 1

# Zapisywanie plików
stanowiska_df = stanowiska_df[["ID Stanowiska", "stanowisko"]]
firmy_df = firmy_df[["ID Firmy", "firma"]]
waluty_df = waluty_df[["ID Waluty", "waluta"]]
źródła_df = źródła_df[["ID Źródła", "źródło"]]
kategorie_df = kategorie_df[["ID Kategoria", "kategoria"]]

stanowiska_df.to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/stanowiska.csv", sep=';', index=False)
firmy_df.to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/firmy.csv", sep=';', index=False)
kategorie_df.to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/kategorie.csv", sep=';', index=False)
waluty_df.to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/waluty.csv", sep=';', index=False)
źródła_df.to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/źródła.csv", sep=';', index=False)
df[["ID Umiejętności", "umiejętności"]].to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/umiejętności.csv", sep=';', index=False)

"""**Tabela faktu i podsumowanie**"""

def summarize_requirements(requirements_text, sentences_count=5):
    # tłumaczenie na angielski
    translator = Translator()
    english_text = [translator.translate(sentence, src='auto', dest='en').text for sentence in requirements_text]

    # Podsumowanie
    english_parser = PlaintextParser.from_string(" ".join(english_text), Tokenizer("english"))
    english_summarizer = LsaSummarizer()
    english_summary = english_summarizer(english_parser.document, sentences_count)

    return " ".join(str(sentence) for sentence in english_summary)

# Tabela faktu
df["ID Oferty"] = df["id_oferty"]
df["ID Stanowiska"] = df.groupby("stanowisko").ngroup() + 1
df["ID Firmy"] = df.groupby("firma").ngroup() + 1
df["ID Kategorii"] = df.groupby("kategoria").ngroup() + 1
df["ID Waluty"] = df.groupby("waluta").ngroup() + 1
df["ID Źródła"] = df.groupby("źródło").ngroup() + 1
df["Link"] = df["link_oferty"]
df["Seniority"] = df["stanowisko"]
df["Wynagrodzenie MIN"] = df["wynagrodzenie_minimalne"]
df["Wynagrodzenie MAX"] = df["wynagrodzenie_maksymalne"]
df["ID Umiejętności"] = df.groupby("umiejętności").ngroup() + 1

# Dodanie kolumny "Podsumowanie"
df["Podsumowanie"] = df["wymagania"].apply(lambda x: summarize_requirements(x, sentences_count=5))

# Zapisywanie do pliku
df[["ID Oferty", "ID Stanowiska", "ID Firmy", "ID Kategorii", "ID Waluty", "ID Źródła", "Link", "ID Umiejętności", "Seniority", "Wynagrodzenie MIN", "Wynagrodzenie MAX", "Podsumowanie"]].to_csv("/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/oferta.csv", sep=';', index=False)

"""**Szukanie adresów firm**"""

def get_location(company_name):
    # Nominatim API
    base_url = "https://nominatim.openstreetmap.org/search"
    # Parametry
    params = {
        "q": company_name,
        "format": "json",
        "addressdetails": 1,
        "limit": 1,  # 1 wynik
        "countrycodes": "pl"  # Polska
    }

    try:
        # Zapytanie
        headers = {"User-Agent": "Your-App-Name"}
        response = requests.get(base_url, params=params, headers=headers)
        # Sprawdzenie statusu
        if response.status_code == 200:
            data = response.json()
            # Szukanie wyników
            if data:
                address = data[0]["display_name"]
                return address
            else:
                return "brak informacji"
        else:
            print(f"Error: {response.status_code}")
            return "brak informacji"
    except Exception as e:
        print("Error:", e)
        return "brak informacji"

# Nazwy firm
company_names = []
with open('/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/firmy.csv', mode='r', newline='') as csvfile:
    reader = csv.DictReader(csvfile)
    fieldnames = reader.fieldnames + ['lokalizacja']
    rows = list(reader)
    for row in rows:
        company_names.append(row['firma'])

# Szukanie adresu dla każdej firmy
output_file = '/content/drive/My Drive/Ekstrakcja_danych/Projekt_2/firmy_lokalizacja.csv'
with open(output_file, mode='w', newline='') as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()

    for row in rows:
        company_name = row['firma']
        location = get_location(company_name)
        if location:
            print(f"Location for {company_name}:")
            print(f"Address: {location}")
            row['lokalizacja'] = location
        else:
            print(f"No location found for {company_name}")
            row['lokalizacja'] = "brak informacji"

        writer.writerow(row)

print("Locations updated in", output_file)