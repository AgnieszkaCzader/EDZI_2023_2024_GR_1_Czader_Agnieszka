
import requests
from bs4 import BeautifulSoup
import time
import json
from google.colab import files
import robotexclusionrulesparser
import sys
import urllib.parse
import re

from google.colab import drive
drive.mount('/content/drive')


headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3', 'Accept-Language': 'en-US,en;q=0.9'}

# Sprawdzenie z plikiem robots.txt
def is_allowed_by_robots_txt(url):
    rp = robotexclusionrulesparser.RobotExclusionRulesParser()
    rp.fetch(url + '/robots.txt')
    return rp.is_allowed(url, '*')

# Wyszukiwanie na stronie rotten tomatoes
def rotten_tomatoes(search_url, film_title):
    encoded_title = urllib.parse.quote(film_title)
    response = requests.get(search_url + encoded_title, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    film_part = soup.find('search-page-result', type='movie')

    if film_part is None:
        print("Brak : ", film_title)
        return
    
    filmy = film_part.select('search-page-media-row')

    rate = None

    for film in filmy:
        title = film.find('a', slot='title')

        if title.text.strip() == film_title.strip() and 'tomatometerscore' in film.attrs:
            rate = film['tomatometerscore']
            return rate.strip()

    return None

def search_rotten_tomatoes_review(title):
    search_url = 'https://www.rottentomatoes.com/search?search='
    rating = rotten_tomatoes(search_url, title)
    return {"ocena_rotten_tomatoes": rating}

# Strona IMDb
url = "https://www.imdb.com/chart/top/"

# User-Agent
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
}

# Lista do przechowywania danych o filmach
filmy_data = []

count = 0

try:
    # Sprawdzenie zgodności z plikiem robots.txt
    if not is_allowed_by_robots_txt("https://www.imdb.com"):
        print("Strona IMDb nie jest zgodna z plikiem robots.txt")
        sys.exit()


    # Wysłanie zapytania GET
    response = requests.get(url, headers=headers)
    response.raise_for_status()

    # Analiza kodu HTML
    soup = BeautifulSoup(response.content, 'html.parser')

    # Znalezienie wszystkich elementów
    film_links = soup.find_all('a', class_='ipc-title-link-wrapper')

    # Tytuły filmów
    for index, link in enumerate(film_links, start=1):
        title = link.h3.text.strip()
        title = f"{title.split('.')[1].strip()}"
        ranking_imdb = index
        rating_element = link.find_next('span', class_='ipc-rating-star')
        ocena = float(rating_element.text.strip().split()[0]) if rating_element else None

        # Szukanie recenzji na Rotten Tomatoes
        rotten_tomatoes_ratings = search_rotten_tomatoes_review(title)

        count += 1
        if count == 101:
            break

        # Zapisywanie danych
        film_data = {
            "tytul_filmu": title,
            "ranking_imdb": ranking_imdb,
            "ocena_imdb": ocena,
            **rotten_tomatoes_ratings
        }

        filmy_data.append(film_data)

        # Opóźnienie
        time.sleep(2)

    # Zapisanie danych do pliku JSON
    with open('filmy_dane.json', 'w') as json_file:
        json.dump(filmy_data, json_file, indent=4)

    # Pobranie pliku JSON na komputer
    files.download('filmy_dane.json')
    print("Dane zapisano do pliku movies_data.json")

except requests.exceptions.RequestException as e:
    print(f"Wystąpił błąd podczas pobierania strony IMDb: {e}")
except Exception as e:
    print(f"Wystąpił nieoczekiwany błąd: {e}")
