# -*- coding: utf-8 -*-
"""ipracuj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H_1uf_Yhj-KobmIEDxvBlsGxWqzUZoNg
"""

import urllib.parse
import requests
from bs4 import BeautifulSoup
import time
import re
import json

# Analiza wynagrodzenia

def parse_salary(salary_text):
    # Wyrażenie regularne do dopasowania wartości liczbowych
    regex_salary = r'(\d[\d\s]*)'

    # Sprawdzenie, czy w tekście występuje VAT
    regex_vat = r'VAT'
    vat_present = re.search(regex_vat, salary_text) is not None

    # Dopasowanie wartości liczbowych
    salaries = re.findall(regex_salary, salary_text)

    # Usuwanie znaków specjalnych, spacji
    salaries = [int(s.replace('\xa0', '').replace(' ', '')) for s in salaries if s.strip()]

    # Mnożenie razy 168 jeśli wynagrodzenie jest podane w godz.
    if 'godz' in salary_text:
        salaries = [salary * 168 for salary in salaries]

    # Dodanie podatku jeśli wynagrodzenie jest w netto
    if vat_present:
        salaries = [int(salary * 1.23) for salary in salaries]

    # Sprawdzenie waluty
    if 'zł' in salary_text:
        currency = 'zł'
    elif 'USD' in salary_text:
        currency = 'USD'
    elif 'EUR' in salary_text:
        currency = 'EUR'
    else:
        currency = 'inna'

    # Ustawienie, że pierwsza wartość jest minimalna, a druga maksymalna
    if len(salaries) == 2:
        min_salary, max_salary = salaries
    elif len(salaries) == 1:
        min_salary = salaries[0]
        max_salary = None
    else:
        min_salary = max_salary = None

    return min_salary, max_salary, currency

# Analiza wymagań

def parse_requirements(requirement_items):
    requirements = []
    for item in requirement_items:
        try:
            decoded_text = item.text.strip().encode('latin1', 'ignore').decode('utf-8', 'ignore')    # dekodowanie tekstu
        except UnicodeEncodeError:
            decoded_text = item.text.strip()
        requirements.append(decoded_text)
    return requirements

# Analiza wymaganych programów

def parse_technologies(tech_list_element):
    # Pusta lista na technologie
    technologies = []

    # Znalezienie programów
    tech_elements = tech_list_element.find_all('span', class_='tjpooi5', attrs={'data-test': 'technologies-item'})

    # Iteracja
    for tech_element in tech_elements:
        # Dodanie do listy
        technologies.append(tech_element.text.strip())

    return technologies

# Główna funkcja - wyszukanie danych i stworzenie listy

def find_job_details(search_url, job_title, max_pages=1):
    all_job_details = []
    for page in range(1, max_pages + 1):
        response = requests.get(f"{search_url}{urllib.parse.quote(job_title)}&pn={page}")
        soup = BeautifulSoup(response.text, 'html.parser')

        # wynagrodzenie
        salary_elements = soup.find_all('span', class_='s1jki39v', attrs={'data-test': 'offer-salary'})
        # link do oferty
        offer_links = soup.find_all('a', class_='core_n194fgoq', attrs={'data-test': 'link-offer'})
        # nazwa firmy
        company_names = soup.find_all('h4', class_='tiles_e1zyaun size-caption core_t1rst47b', attrs={'data-test': 'text-company-name'})
        # id oferty
        offer_ids = soup.find_all('div', class_='be8lukl core_po9665q', attrs={'data-test': 'default-offer'})
        # stanowisko
        positions = soup.find_all('li', class_='mobile-hidden tiles_iwlrcdk', attrs={'data-test': 'offer-additional-info-0'})
        # tytuł oferty
        offer_titles = soup.find_all('h2', class_='tiles_b1yuv00i', attrs={'data-test': 'offer-title'})
        # programy
        technologies_lists = soup.find_all('div', class_='b1fdzgc4', attrs={'data-test': 'technologies-list'})

        job_details = []

        for i, (salary_element, link_element, company_element, id_element, position_element, title_element, tech_list_element) in enumerate(zip(salary_elements, offer_links, company_names, offer_ids, positions, offer_titles, technologies_lists)):
            salary_text = salary_element.text.strip()
            salary_text2 = salary_element.text.strip().replace('\xa0', ' ')    # ogólna informacja o wynagrodzeniu
            min_salary, max_salary, currency = parse_salary(salary_text)       # minimalne i maksymalne wynagrodzenie

            # sposób wyświetlania danych
            link = link_element['href']
            company_name = company_element.text.strip()
            offer_id = id_element['data-test-offerid']
            position = position_element.text.strip()
            offer_title = title_element.text.strip()

            # informacja o źródle
            source = urllib.parse.urlparse(link).hostname

            # Pobranie informacji o programach
            technologies = parse_technologies(tech_list_element)

            # Wchodzenie na strony z ofertami pracy - pobranie danych o wymaganiach
            offer_response = requests.get(link)
            offer_soup = BeautifulSoup(offer_response.text, 'html.parser')

            # Pobranie informacji o wymaganiach
            requirements_section = offer_soup.find('div', class_='offer-viewfjH4z3', attrs={'data-test': 'section-requirements-expected'})
            requirements = []
            if requirements_section:
                requirement_items = requirements_section.find_all('p', class_='offer-viewchej5g')
                requirements = parse_requirements(requirement_items)

            # Zebranie danych
            job_details.append({
                "link_oferty": link,
                "id_oferty": offer_id,
                "tytuł_oferty": offer_title,
                "firma": company_name,
                "stanowisko": position,
                "wynagrodzenie": salary_text2,
                "wynagrodzenie_minimalne": min_salary,
                "wynagrodzenie_maksymalne": max_salary,
                "waluta": currency,
                "źródło": source,
                "wymagania": requirements,
                "Array": technologies,
                "kategoria": "BigData/Data Science"})

        all_job_details.extend(job_details)

        # Dodanie opóźnienia (1 sekudna)
        time.sleep(1)

    return all_job_details

# Ustawienie linku do strony

def search_job_details(job_title, max_pages=4):
    search_url = 'https://it.pracuj.pl/praca/krakow;wp?rd=0&et=17%2C18%2C4&sal=1&its=big-data-science&q='
    job_details = find_job_details(search_url, job_title, max_pages)
    return job_details

result = search_job_details("", max_pages=4)

# Zapisanie pliku z danymi

def save_to_json(data, filename='job_details_pracuj.json'):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Dane zostały zapisane do pliku: {filename}")

# Pobranie pliku z danymi

def download_json(filename='job_details_pracuj.json'):
    from google.colab import files
    files.download(filename)
    print("Plik JSON został pobrany na Twój komputer.")

# Utworzenie raportu - zliczenie programów

def generate_technology_report(job_details):
    technology_count = {}

    # Sprawdzenie każdej oferty pracy
    for job_detail in job_details:
        # Pobranie listy programów dla każdej oferty
        technologies = job_detail.get("Array", [])

        # Zsumowanie programów
        for technology in technologies:
            technology_count[technology] = technology_count.get(technology, 0) + 1

    return technology_count

# Utworzenie raportu - zliczenie stanowisk i płac

# Informacje do raportu
def generate_position_report(job_details):
    position_count = {
        "Junior Data Engineer": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "[Mid/Regular] Data Engineer": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Senior Data Engineer": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Junior Data Analyst": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "[Mid/Regular] Data Analyst": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Senior Data Analyst": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Junior Data Scientist": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "[Mid/Regular] Data Scientist": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Senior Data Scientist": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Junior Data Architect": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "[Mid/Regular] Data Architect": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None},
        "Senior Data Architect": {"count": 0, "min_salary": None, "max_salary": None, "average_salary": None}
    }

    # Sprawdzenie każdej oferty pracy
    for job_detail in job_details:
        # Pobranie tytułów ofert pracy
        title = job_detail.get("tytuł_oferty", "")

        # Pobranie wynagrodzeń minimalnych i maksymalnych
        min_salary = job_detail.get("wynagrodzenie_minimalne")
        max_salary = job_detail.get("wynagrodzenie_maksymalne")

        # Zliczenie wystąpienia każdego stanowiska i sprawdzenie wynagrodzenia minimalnego i maksymalnego
        for position, position_info in position_count.items():
            if position in title:
                position_info["count"] += 1
                if position_info["min_salary"] is None or (min_salary is not None and min_salary < position_info["min_salary"]):
                    position_info["min_salary"] = min_salary
                if position_info["max_salary"] is None or (max_salary is not None and max_salary > position_info["max_salary"]):
                    position_info["max_salary"] = max_salary

    # Obliczenie średniego wynagrodzenie dla każdego stanowiska
    for position, position_info in position_count.items():
        if position_info["count"] > 0:
            if position_info["max_salary"] is not None:
                position_info["average_salary"] = (position_info["min_salary"] + position_info["max_salary"]) / 2
            else:
                position_info["average_salary"] = position_info["min_salary"]

    return position_count

# Zapisanie raportu

def save_to_json(data, filename='technology_report_pracuj.json'):
    with open(filename, 'w', encoding='utf-8') as file:
        json.dump(data, file, ensure_ascii=False, indent=4)

# Pobranie raportu

def download_json(filename='technology_report_pracuj.json'):
    from google.colab import files
    files.download(filename)
    print("Plik JSON został pobrany na Twój komputer.")

# Wywołania funkcji

job_details = search_job_details("", max_pages=4)
technology_report = generate_technology_report(result)

save_to_json(technology_report, filename='technology_report_pracuj.json')

# Tworzenie raportu z informacją z informacjami o stanowiskach
position_report = generate_position_report(result)

# Dodanie raportu z informacjami o stanowiskach do raportu z informacjami o programach
for position, count in position_report.items():
    technology_report[position] = count

# Zapisanie zaktualizowanego raportu
save_to_json(technology_report, filename='technology_report_pracuj.json')

# Zapisanie i pobranie plików
save_to_json(job_details, filename='job_details_pracuj.json')
download_json(filename='technology_report_pracuj.json')
download_json(filename='job_details_pracuj.json')