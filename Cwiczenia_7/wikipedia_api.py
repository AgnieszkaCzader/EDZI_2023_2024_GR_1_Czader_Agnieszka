# -*- coding: utf-8 -*-
"""Wikipedia_API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r_GlmFC_pB9zmBnI2w5jEMtezwOIoDRi

# Pobranie artykułu
"""

import requests
from google.colab import drive

# Podłączanie dysku Google
drive.mount('/content/drive')

def get_wikipedia_article_text(article_title):
    # Adres API
    endpoint = "https://en.wikipedia.org/w/api.php"

    article_params = {
        "action": "query",
        "format": "json",
        "prop": "extracts",
        "titles": article_title,
        "explaintext": True # Return plain text instead of HTML
    }
    article_response = requests.get(endpoint, params=article_params)
    if article_response.status_code == 200:
        article_data = article_response.json()
        # Ekstrakcja tekstu z artykułu
        page_id = next(iter(article_data["query"]["pages"]))
        if "extract" in article_data["query"]["pages"][page_id]:
            article_text = article_data["query"]["pages"][page_id]["extract"]
            return article_text
        else:
            print("No extract found in article data")
            return None
    else:
        print("Failed to fetch article text")
        return None

# Pobranie artykułu
article_title = "British Shorthair"  # Tytuł artykułu
article_text = get_wikipedia_article_text(article_title)
if article_text:
    with open("/content/drive/My Drive/Ekstrakcja_danych/Wikipedia_API/org.txt", "w", encoding="utf-8") as file:
        file.write(article_text)
    print("Article text saved to org.txt")
else:
    print("Failed to fetch article text")

"""# Podsumowanie za pomocą sumy"""

# Funkcja
def summarize_article(article_text, language="english", sentences_count=5):
    with open(article_text, "r", encoding="utf-8") as file:
        text = file.read()
    parser = PlaintextParser.from_string(text, Tokenizer(language))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, sentences_count)
    return summary

# Podsumowanie artykułu
article_text = '/content/drive/My Drive/Ekstrakcja_danych/Wikipedia_API/org.txt'
if article_text:
    summary = summarize_article(article_text)
    with open("/content/drive/My Drive/Ekstrakcja_danych/Wikipedia_API/outcome_sumy.txt", "w", encoding="utf-8") as file:
        for sentence in summary:
            file.write(str(sentence) + "\n")
    print("Summary saved to outcome.txt")
else:
    print("Failed to fetch article text")

"""# Podsumowanie za pomocą bert-extractive-summarizer"""

!pip install bert-extractive-summarizer
from summarizer import Summarizer

# Ścieżka do pliku
file_path = '/content/drive/My Drive/Ekstrakcja_danych/Wikipedia_API/org.txt'

# Wczytanie zawartości pliku
with open(file_path, 'r', encoding='utf-8') as file:
    body = file.read()

# Model Summarizer
model = Summarizer()

# Generowanie podsumowania
result = model(body, min_length=60, ratio=0.2)
full_summary = ''.join(result)

# Zapisanie podsumowania
output_path = '/content/drive/My Drive/Ekstrakcja_danych/Wikipedia_API/outcome_bert_extractive_summarizer.txt'
with open(output_path, "w", encoding="utf-8") as file:
    file.write(full_summary)

print("Summary saved to outcome_bert_extractive_summarizer.txt")